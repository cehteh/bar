
  bar -- BAshRulez the bash rule evaluator

ABOUT

  Bar is a command runner that determines the order of commands and which ones to run, based
  on rules. It lets the user define rules that dependently and conditionally evaluate shell
  statements.  This is similar to other tools such as 'make' or 'just' but adds some
  opinionated differences. There is support for using bar as driver for githooks for
  automating workflows.

  The notable differences to other similar tools are:

  * Rules in bar can have multiple clauses.
  * Normally rules are conjunctive, when a clause of a rule fails then the rule fails and not
    further evaluated.
  * Rules can be made disjunctive, then a rule succeeds on the first succeeding clause.
  * Clauses can be conditionally skipped.
  * The results of rules are cached. Normally rules are evaluated only once, although there is
    a form to define rules that always evaluate.
  * We have a module that can memorize commands persistently and schedule evaluation to the
    background. A later instance can pick up the results from this background evaluation.

  When you read this because you have seen 'bar' or '.bar' used in a repository then you may
  look at INITIAL INSTALLATION below.


QUICKSTART

  When you cloned a project that uses bar then you can:

   ./bar                # just runs whatever the maintainer decided as default
   ./bar watch          # watches the project for changes and runs the default
   ./bar watch fastchk  # watches the project for changes and runs fast checks
   ./bar activate       # installs githooks and other local prep work

  When you want to install and use it in your own projects:

   git clone https://seed.pipapo.org/z3WhBdHt1VVNyeQ61zpY66pNzuSrP.git bar
   cd bar
   ./bar init_install ~/.local/
   git pull   # to get updates

  Then you can initialize it in your projects with:

   bar init
   git add Barf Bar.d .gitignore
   git commit -m 'start using bar'

   bar init_update  # for updating the version stored in the project

  The second personality of bar is 'please' eg.:

   please wake <hostname>     # sends a wake-on-lan packet to a host
   please suspend <hostname>  # remotely sends a host to suspend-to-ram

  Buildin help system:

   bar help [topic]
   ./bar help [topic]


INVOCATION AND SEMANTICS

   bar [rulefile] [--bare] [rule [arguments..]]
   please [rulefile] [--bare] [rule [arguments..]]
   ./bar [rulefile] [--bare] [rule [arguments..]]
   <symlink> [arguments..]

  Starts by loading all initial modules (those which have underscores in their name) and the
  rulefile, which is either "Barf", "barf", ".Barf", ".barf" when called as 'bar' or "Pleasef"
  "pleasef" ".Pleasef" ".pleasef" "$HOME/.Pleasef" when called as 'please' and when the first
  argument is not a file.

  The duality of having 'bar' installed as 'please' is not only for making it more
  polite. This allows for a user to have private '.Pleasef' defining rules that are for common
  administration tasks distinct from versioned per-project 'Barf' and 'Bar.d' rules.

  Note that the rules for looking up the Barf/Pleasef and the modules are somewhat distinct.
  Nevertheless modules are in either way only loaded from a single directory and are not
  located in different paths. This is a opinionated choice to make the module system
  self-contained and not depend on secondary locations.

  Then the given rule (or MAIN) with the supplied arguments becomes evaluated. Finally, if
  a 'CLEANUP' rule exists, it is evaluated.

  The exit code of the invocation is the exit code of the main rule.

  Just calling 'bar' without any arguments will evaluate the 'MAIN' rule which should be what
  people expect by default. Other useful rules like 'help' for this documentation are defined
  by modules.

  When called by a symlink, the standard rulefile is loaded and instead of ‘MAIN’, a rule with
  the same name as the symlink is called with all arguments passed.


MODULE API/RULES

  cargo - Rust/cargo support

    Variables:

      CARGO_TOOLCHAIN (xg)
        The rust toolchain that is used for most operations.
        Default: ${CARGO_TOOLCHAIN:-}

    Functions:

      is_cargo_tool_installed [+toolchain] <tool> [args..]
        Checks if 'cargo <tool>' is installed
        When [args..] is not present then '--version' is used. Tools that do not handle
        '--version' need 'args..' to give some sensible result.

      is_cargo_toolchain_available <+toolchain>
        Check if a toolchain is available

      cargo_tool_complete List available cargo tools/subcommands for completion
        This function provides completion for cargo tool names (subcommands).
        It lists both built-in cargo commands and installed extensions.

      cargo_fix Run 'cargo fix'.
        Note that this will modify the source in place.

      cargo_build [buildargs..]
        Compile a local package and all of its dependencies

      cargo_test [testargs..]
        Execute all unit and integration tests

      cargo_check Run 'cargo check'. Testing whenever the source can be compiled.

      cargo_clippy_errors Tests with 'cargo clippy' for errors only

      cargo_clippy_strict Test with 'cargo clippy' for warnings and errors

      cargo_doc [docargs..]
        Build this package's and its dependencies' documentation

      cargo_update Run 'cargo update'

      cargo_has_unsafe_code Checks if the source use 'unsafe' code.
        This does only a coarse '-Funsafe-code' check, but does not depend on external tools.

      cargo_miri [miriargs..]
        Run tests under miri (memory sanitizer)

      cargo_bench Run 'cargo bench'

      cargo_publish Run 'cargo publish'

    Rules:

      cargo_lint
        Runs all linters

      cargo_toolchain <+toolchain>
        Clause local change of the toochain used by cargo.

      cargo_fmt_check
        Checks whenever the source code is well formatted.
        If available the '+nightly' toolchain is used.

      cargo_build_unit_tests
        Builds unit tests.

      cargo_test_units
        Runs unit tests.

      cargo_build_integration_tests
        Build integration tests

      cargo_test_integrations
        Run integration tests. This includes the doctests.

      cargo_fmt
        Run 'cargo fmt'. When available the '+nightly' toolchain is used.
        Note that this will modify the source in place.

      cargo_miri
        When there is unsafe code then run a test under miri supervision.
        Requires the '+nightly' toolchain.

      cargo_check_msrv
        Checks whenever the 'rust_version' in the manifest is sufficient.
        This check only happens when 'cargo-msrv' is installed, otherwise a warning is printed.

      cargo_mutants
        Run 'cargo mutants' when it is available.
        When 'cargo-mutants' is not installed a warning is printed.

      cargo_outdated
        Run 'cargo outedated' when it is available.
        When 'cargo-outdated' is not installed a warning is printed.

      cargo_audit
        Run 'cargo audit' when it is available.
        When 'cargo-audit' is not installed a warning is printed.

      cargo_semver_checks
        Run cargo semver-checks to test if a semver for a release is sufficient

  cargo_rules - General support rules for Rust/Cargo projects
    
    Checks whenever we are in a cargo project.
    Hooks cargo support into the 'std_rules'

    Functions:

      has_cargo_manifest Checks if 'Cargo.toml' exists.

    Rules:

      is_cargo_installed
        Successful when 'cargo' is available.

      is_cargo_project
        Check whenever we are ready to use cargo here.

      release_publish
        Push a software release to crates.io (or whatever registry is configured).

  cliff - git cliff changelog generation

    Variables:

      GIT_CLIFF_OUTPUT (gx)
        Output where to write the changelog
        Default: ${GIT_CLIFF_OUTPUT:-CHANGELOG.md}

    Functions:
        The generated changelog and cliff.toml (when there where changes) are implicitly staged.
        This does not git-commit though.

    Rules:

      cliff_changelog [cliff_args..]
        use git cliff to generate a changelog.

      cliff_release_changelog
        Makes a changelog for a release, commits and tags it. Must be called from a release worktree
        with state 'tested'.

      cliff_changelog_amend
        Regenerates the changelog up to the previous commit and amends the current commit with the new changelog

  cliff_rules - Hooks git cliff into build_versioned_artifacts

    Rules:

      release_changelog
        Create a cliff changelog for releases when there is a 'cliff.toml' at the project root.

  entr - Support for the entr file watcher

    Functions:

      entr_watch_git_index Defaults to 'MAIN'.

    Rules:

      is_entr_installed
        Succeeds when 'entr' is available.

      entr_watch
        Runs bar in a loop again whenever a file gets changed.
        Takes a target rule and its parameters as argument.
        Defaults to 'MAIN'.

  entr_rules - hooks entr into the watch/watch_git_index rules

  git - Git support rules

    Rules:

      is_git_repository
        Succeeds when we are in inside (at or below) a git repository.

      is_git_toplevel
        Succeeds when we are at the top level of a git repository.

      git_is_clean
        Checks if there are modified files, fails if there are any.

      git_branch_matches [branchpattern..]
        Checks that the current git branch matches any of the given patterns.

      is_git_pre_commit_hook
        Checks if called as githook.

      is_git_pre_merge_commit_hook
        Checks if called as githook.

      is_git_prepare_commit_msg_hook
        Checks if called as githook.

      is_git_commit_msg_hook
        Checks if called as githook.

      is_git_applypatch_msg_hook
        Checks if called as githook.

      is_git_pre_applypatch_hook
        Checks if called as githook.

      is_git_post_applypatch_hook
        Checks if called as githook.

      is_git_post_commit_hook
        Checks if called as githook.

      is_git_pre_rebase_hook
        Checks if called as githook.

      is_git_post_checkout_hook
        Checks if called as githook.

      is_git_post_merge_hook
        Checks if called as githook.

      is_git_pre_push_hook
        Checks if called as githook.

      is_git_pre_receive_hook
        Checks if called as githook.

      is_git_update_hook
        Checks if called as githook.

      is_git_proc_receive_hook
        Checks if called as githook.

      is_git_post_receive_hook
        Checks if called as githook.

      is_git_post_update_hook
        Checks if called as githook.

      is_git_reference_transaction_hook
        Checks if called as githook.

      is_git_push_to_checkout_hook
        Checks if called as githook.

      is_git_pre_auto_gc_hook
        Checks if called as githook.

      is_git_post_rewrite_hook
        Checks if called as githook.

      is_git_sendemail_validate_hook
        Checks if called as githook.

      is_git_fsmonitor_watchman_hook
        Checks if called as githook.

      is_git_post_index_change_hook
        Checks if called as githook.

      is_git_main_branch
        Checks if the current branch matches 'main master'.

      is_git_release_branch
        Checks if the current branch matches 'release-* release/*'.

      is_git_devel_branch
        Checks if the current branch matches 'devel-* devel/*'.

      is_git_feature_branch
        Checks if the current branch matches 'feature-* feature/*'.

      is_git_bugfix_branch
        Checks if the current branch matches 'bugfix-* bugfix/* fix-* fix/*'.

      is_git_hotfix_branch
        Checks if the current branch matches 'hotfix-* hotfix/*'.

      is_git_improvement_branch
        Checks if the current branch matches 'improvement-* improvement/*'.

      is_git_doc_branch
        Checks if the current branch matches 'doc-* doc/*'.

      is_git_wip_branch
        Checks if the current branch matches 'wip-* wip/*'.

      is_git_experimental_branch
        Checks if the current branch matches 'experiment-* experiment/*'.

  git_lib - Git support library

    Functions:

      git_toplevel Returns the path to the toplevel git repository.

      git_bar Fails when there is no bar initialized.

      git_dir Returns the absolute path to the 'GIT_DIR' (.git/).
        Note that this may be distinct from '$(git_toplevel)/.git/' when worktrees or other git
        features are used.

      git_ls_files [git-ls-files-opts]..
        Sorted list of files in the git repository.
        git-ls-files '--cached' and '--exclude-standard' are always implied.  Notable options
        are '-z' for zero terminated output and '--others' to show untracked files.

      git_branch_name Returns the branch name that is checked out, dies when not in a git branch.

      git_branch_complete List all git branches for completion
        This function is called by bar_complete to provide completions for branch parameters.

      git_branch_find [patterns]..
        lists branches matching the given patterns

      git_branch_find_one [patterns]..
        lists the matching the given patterns, fails when there are more than one

      git_is_ancestor [patterns]..
        checks if the current branch is a ancestor of the one branch matching the patterns

      git_tree_hash [git-ls-files-opts]..
        Returns a sha1 hash over the current directory
        This hash is a identifier used by bar. It is not the same hash git using for storing trees.

      git_add_ignore [patterns..]
        Adds new ignore patterns to '.gitignore'.
        Patterns that are already present are skipped.

      git_parse_worktrees [main [dirs [branches [heads]]]]
        Parses git worktrees into variables provided by the caller
        - main:     variable storing the main worktree (dir)
        - dirs:     associative array storing 'branch:directory'
        - branches: associative array storing 'directory:branch'
        - heads:    associative array storing 'directory:head'

  git_rules - Standard rules for git support

    Rules:

      pre-commit
        Entry rule when called as githook.

      pre-merge-commit
        Entry rule when called as githook.

      prepare-commit-msg
        Entry rule when called as githook.

      commit-msg
        Entry rule when called as githook.

      applypatch-msg
        Entry rule when called as githook.

      pre-applypatch
        Entry rule when called as githook.

      post-applypatch
        Entry rule when called as githook.

      post-commit
        Entry rule when called as githook.

      pre-rebase
        Entry rule when called as githook.

      post-checkout
        Entry rule when called as githook.

      post-merge
        Entry rule when called as githook.

      pre-push
        Entry rule when called as githook.

      pre-receive
        Entry rule when called as githook.

      update
        Entry rule when called as githook.

      proc-receive
        Entry rule when called as githook.

      post-receive
        Entry rule when called as githook.

      post-update
        Entry rule when called as githook.

      reference-transaction
        Entry rule when called as githook.

      push-to-checkout
        Entry rule when called as githook.

      pre-auto-gc
        Entry rule when called as githook.

      post-rewrite
        Entry rule when called as githook.

      sendemail-validate
        Entry rule when called as githook.

      fsmonitor-watchman
        Entry rule when called as githook.

      post-index-change
        Entry rule when called as githook.

      release_publish
        Push tags and branches to the default push remove when publishing a release.

  githook - Support library for managing githooks

    Rules:

      githook_enable
        Takes a list of githooks to enable as parameters. Must be called from the git_toplevel.
        Fails when a hook is not available.

      githook_disable
        Takes a list of githooks to disable as parameters. Must be called from the git_toplevel.
        Fails when a hook is not pointing to bar.

  help - Provides help, extracts documentation from modules
    
    Inline documentation uses two or three hash characters. Three '###' is used for file level
    documentation to describe a file entirely. Two '##' are used in different contexts:
    
    Variable definitions that use the 'declare' keyword can use '##' at the end of the same
    line to document the variable.
    
    Functions use formal parameter syntax: 'function name ## [--opt] <required> [optional] - description'
    Parameters use: <mandatory>, [optional], param.., [param..], --flags, foo|bar alternatives.
    Multi-line documentation continues on following lines starting with optional whitespace
    followed by '##' and parameter explanations.
    
    Rules are documented by '##' lines before the rule definition, using the same formal
    parameter syntax when the rule accepts parameters.
    
    Only variables, functions and rules that have doc comments are included in the output.

    Rules:

      help [[-s|--short] topic|rule|function|variable|text]
        Show help topics
        - [-s|--short]:                 Show only the queried sections
        - topic|rule|function|variable: Navigate to or show only specified topics
        - text:                         Search the specified text in the documentation

  init - Module to install, initialize and update bar itself

    Functions:

      init_update_merge_barf when 'Barf' was not or only slightly edited.

      init_install [prefix]
        install links in [prefix] or '~/.local'.
        Bar will usually exist somewhere in a users home directory where it got checked
        out. Bar does rolling releases, it is continuously improved. To make this checked out
        version available for use this 'init_install' will create symlinks at the given
        [prefix] directory or in '~/.local' by default.

    Rules:

      init
        Initialize bar in a workdir. This copies the main 'bar' script, a default 'Barf' and all
        'Bar.d/' modules to current directory. When the workdir is a git repository then
        '.gitignore' is updated to exclude the files and directories 'bar' will use for
        temporary/testing storage. A user may edit the 'Barf' file to suit the project/personal
        preference.  Unneeded modules in 'Bar.d' can be deleted. When that is done the new/changed
        files should be committed to version control.

      init_update_existing
        Updates a initialized working tree. That is 'bar' itself will be updated when a newer
        version is available and any module in 'Bar.d/' that exists in the local working tree will
        be updated as well.  Barf will be left untouched.

      init_update
        Updates a initialized working tree. That is 'bar' itself will be updated when a newer
        version is available and all modules including ones that are not present in 'Bar.d/' will
        be updated as well.  Barf will be left untouched.

  lock - Library for managing lockfiles

    Functions:

      lock_wait <lockname>
        Waits until we have the lock on a lockfile.
        '.lock' is automatically appended to the 'lockname'.
        These locks are recursive the same process can lock_wait on the same lockfile multiple
        times which must be paired with the same numbers of 'lock_remove' to unlock it.

      lock_try_norec <lockname>
        try to lock a lockfile non recursively.
        Will not wait, fails when we already have the lock
        Locks 'new' and then removes the 'old' locck.

      lock_send <lock> <who>
        Sends a 'lock' to another process with pid 'who'.
        Locks can be send atomically to another process. 'lock_send' registers the lock to be
        send to a process, the lock will be send at the final lock_remove in the sending
        process when all recursive locks are freed. Note that to obtain the 'who' pid the
        receiver has to be started first.

      lock_receive <lock>
        Wait for lock send by another process.
        Blocks until the the sender removed all uses of 'lock',

      lock_remove <lock>
        Unlocks 'lock'.
        'lock_remove' must be paired with the same numbers of earlier 'lock_wait'. Only the
        last 'lock_remove' will release the lockfile.

  memodb - A persistent database which memoizes command results
    
    It records the stdout, stderr and state/exitcode commands and functions.  Commands and
    functions that are used with the memodb must be pure as their result must only depend on
    the arguments passed and the directory they are called in. The content of the directory is
    hashed for the database. This requires properly set up '.gitignore' rules to ignore any
    build artifacts. Otherwise subsequent instances will not use the same memodb and re-run
    commands.
    
    This is used for
     1. Persist/cache command results throughout multiple invocations
     2. Implement background processing where one invocation can schedule commands to be
        executed in the background and a later invocation can collect the results

    Variables:

      MEMODB_KEEP (gx)
        How many trees to keep.
        Default: ${MEMODB_KEEP:-5}

      MEMODB_BACKGROUNDING (gx)
        Global backgrounding flag.
        Default: ${MEMODB_BACKGROUNDING:-true}

    Functions:

      memodb_eval <cmd> [args..]
        Execute a command in foreground and stores its results.
        The command is only executed on the first time the 'memodb_eval' is called with the
        same options in the same directory. Any subsequent call will replay the stored results.

      memodb_schedule <cmd> [args..]
        Schedules a command to be run in background.
        Will 'memodb_eval' the command when 'MEMODB_BACKGROUNDING' is not true.

      memodb_result <cmd> [args..]
        Retrieves the background results.
        Waits for the backgrounding to be completed. Merges the results into the current memodb.
        Replays the result (stdout/stderr/exitcode) of the given 'cmd [args..]'.
        'cmd [args..]' must be the same as used when scheduling it to the background.
        Will error when the command was not previously scheduled and backgrounding is enabled.

  net - Low level networking functions

    Functions:

      net_canon_hostname <host>
        Canonalize 'host' to a FQDN.

      net_host_reachable <host> [timeout [tries]]
        checks if 'host' is reachable.
        The check uses ICMP ping to check for reachability.
        The [timeout] defaults to 10 seconds and [tries] defaults to 3.

      net_wakeonlan <host>
        Sends a wake-on-lan signal.
        'host' is a hostname. '/etc/ethers' must have entries to resolve hosts. See
        'contrib/update-ethers.sh' for automatically discover and maintain '/etc/ethers'. Waits
        for the host to become reachable. Fails when it is not reachable.

  podman - Podman container support for Bar
    
    Provides container-based build and test environments with multi-architecture support,
    layered image construction, network isolation, and artifact collection.

    Functions:

      is_podman_installed Check if podman is available on this system

      podman_version Get podman version

      podman_run <image:tag> [--timeout <seconds>] [--platform <arch>] [--network <preset>] [--volume <src:dst>] [command] [args..]
        Run command in container
        Executes a command inside a podman container with various options.
        Options:
          --timeout <seconds>   - Maximum runtime (container killed after timeout)
          --platform <arch>     - Target architecture (e.g., linux/amd64, linux/arm64)
          --network <preset>    - Network configuration (public, private, local, isolated)
          --volume <src:dst>    - Mount volumes (can be specified multiple times)
          --env <VAR=value>     - Set environment variables (can be specified multiple times)
        
        The first non-option argument is the image name, followed by the command and its arguments.

      podman_platform_complete <platform>
        List available platforms for completion

      podman_normalize_arch Normalize architecture name to standard format
        Converts various architecture names to standardized format
        Usage: podman_normalize_arch <arch>

      podman_normalize_platform Normalize platform name to linux/<arch> format
        Converts architecture names to platform format (linux/<arch>)
        Usage: podman_normalize_platform <arch-or-platform>

      is_podman_arch_available <arch>
        Check if an architecture is available (with QEMU if needed)
        Checks if the specified architecture is available for use with podman.
        This includes native architectures and those available via QEMU emulation.

      podman_arch_setup Setup QEMU for cross-architecture support
        Ensures QEMU binfmt support is properly configured for multi-architecture builds.
        This enables running containers for different architectures via emulation.

      is_qemu_available <arch>
        Check if QEMU is available for the specified architecture
        Checks if QEMU emulation is available for the given architecture.

      podman_image_build_multiarch <tag> <platform> <containerfile>
        Build an image for a specific platform
        Builds a container image for a specific platform/architecture.
        Usage: podman_image_build_multiarch <tag> <platform> <containerfile>
        Example: podman_image_build_multiarch myimg:latest linux/arm64 Containerfile

      podman_image_build_matrix <base-tag> <platforms..>
        Build multi-platform images
        Builds the same image for multiple platforms.
        Usage: podman_image_build_matrix <base-tag> <containerfile> <platforms..>
        Example: podman_image_build_matrix myimg:v1 Containerfile linux/amd64 linux/arm64

      podman_image_build <tag> <containerfile>
        Build an image from a Containerfile
        Builds a container image from a Containerfile.
        Usage: podman_image_build <tag> <containerfile>

      podman_run_arch <arch> <image> [--timeout <sec>] [command..]
        Run command for specific architecture
        Executes a command in a container for a specific architecture.
        Automatically sets the correct platform based on the architecture.
        Usage: podman_run_arch <arch> <image> [--timeout <sec>] [command..]

      podman_get_native_arch Get the native architecture
        Returns the native architecture in normalized format (amd64, arm64, arm/v7).

      podman_list_available_archs List all available architectures
        Lists all architectures available for use (native + QEMU).

      podman_image_snapshot <container-id> <tag>
        Snapshot a container to an image
        Creates an image from a container's current state.
        Usage: podman_image_snapshot <container-id> <tag>

      podman_layer_bar <name:tag> <base-image>
        Create base layer with Bar runtime
        Creates a container image with Bar and bash installed.
        Usage: podman_layer_bar <name:tag> <base-image>

      podman_layer_toolchain <name:tag> <parent:tag> <toolchain>
        Add toolchain layer
        Adds a development toolchain to an existing image.
        Usage: podman_layer_toolchain <name:tag> <parent:tag> <toolchain>
        Supported toolchains: rust, python, nodejs, go, gcc

      podman_layer_dev <name:tag> <parent:tag>
        Add development tools layer
        Adds common development tools to an existing image.
        Usage: podman_layer_dev <name:tag> <parent:tag>

      podman_containerfile_from <base-image>
        Start building a Containerfile from base image
        Starts a new Containerfile with the specified base image.
        Usage: podman_containerfile_from <base-image>

      podman_containerfile_run <command>
        Add RUN instruction to Containerfile
        Adds a RUN instruction to the current Containerfile.
        Usage: podman_containerfile_run <command>

      podman_containerfile_copy <src> <dst>
        Add COPY instruction to Containerfile
        Adds a COPY instruction to the current Containerfile.
        Usage: podman_containerfile_copy <src> <dst>

      podman_containerfile_env <key> <value>
        Add ENV instruction to Containerfile
        Adds an ENV instruction to the current Containerfile.
        Usage: podman_containerfile_env <key> <value>

      podman_containerfile_workdir <workdir>
        Add WORKDIR instruction to Containerfile
        Adds a WORKDIR instruction to the current Containerfile.
        Usage: podman_containerfile_workdir <workdir>

      podman_containerfile_build <tag>
        Build image from programmatically generated Containerfile
        Builds an image from the programmatically generated Containerfile.
        Usage: podman_containerfile_build <tag>

      podman_network_create <name> [options..]
        Create a custom podman network
        Creates a custom podman network with optional configuration.
        Usage: podman_network_create <name> [--subnet <cidr>] [--gateway <ip>]

      podman_network_remove <name>
        Remove a podman network
        Removes a podman network.
        Usage: podman_network_remove <name>

      podman_network_exists <name>
        Check if a network exists
        Checks if a podman network exists.
        Usage: podman_network_exists <name>

      podman_network_list List all podman networks
        Lists all podman networks.

      podman_network_preset <name> <preset>
        Configure network with security preset
        Configures a network with a security preset.
        Presets: public, private, local, isolated
        Usage: podman_network_preset <name> <preset>

      podman_port_map <container-port> <host-port> [protocol]
        Format port mapping
        Formats a port mapping for use with podman run.
        Usage: podman_port_map <container-port> <host-port> [protocol]
        Returns: "<host-port>:<container-port>[/protocol]"

      podman_firewall_config <network-name>
        Initialize firewall configuration for a network
        Initializes firewall configuration for a network.
        Usage: podman_firewall_config <network-name>

      podman_firewall_allow_port <network-name> <port> [protocol]
        Allow port through firewall
        Adds a port to the firewall allow list.
        Usage: podman_firewall_allow_port <network-name> <port> [protocol]

      podman_firewall_allow_network <network-name> <cidr>
        Allow network range through firewall
        Adds a network range to the firewall allow list.
        Usage: podman_firewall_allow_network <network-name> <cidr>

      podman_firewall_deny_all <network-name>
        Set default deny policy for firewall
        Sets a default deny policy for the network.
        Usage: podman_firewall_deny_all <network-name>

      podman_firewall_show <network-name>
        Get firewall configuration summary
        Shows the firewall configuration for a network.
        Usage: podman_firewall_show <network-name>
        Creates an artifact directory and returns its path.
        Usage: podman_artifact_dir [path]
        If no path provided, creates a temporary directory.

      podman_artifact_mount <path>
        Prepare artifact mount specification
        Prepares a volume mount specification for artifacts.
        Usage: podman_artifact_mount <path>
        Returns: "<path>:/artifacts:z"

      podman_artifact_fetch <container> <src> <dst>
        Copy artifacts from container to host
        Copies artifacts from a container to the host filesystem.
        Usage: podman_artifact_fetch <container> <src> <dst>
        Example: podman_artifact_fetch my-container /app/target/release/binary ./artifacts/

      podman_artifact_extract_all <container> <dst-dir>
        Extract all artifacts from container's /artifacts directory
        Extracts all files from container's /artifacts directory to host.
        Usage: podman_artifact_extract_all <container> <dst-dir>

      podman_artifact_fetch_tarball <container> <src-path> <dst-dir>
        Fetch and extract tarball from container
        Fetches a tarball from container and extracts it.
        Usage: podman_artifact_fetch_tarball <container> <src-path> <dst-dir>

      podman_run_with_artifacts <image> <rule> <artifact-dir> [args..]
        Run rule and collect artifacts
        Runs a rule in container and collects artifacts automatically.
        Usage: podman_run_with_artifacts <image> <rule> <artifact-dir> [args..]

      podman_container_export <container-id>
        Export container filesystem as tarball
        Exports a container's entire filesystem as a tarball.
        Usage: podman_container_export <container-id> [output-file]
        If no output file specified, outputs to stdout

      podman_config_load <config-name>
        Load container configuration template
        Loads a container configuration from an associative array.
        Usage: Declare a config array, then use: podman_config_load CONFIG_NAME
        The array should be named PODMAN_CONFIG_<config-name> in uppercase.

      podman_run_with_config <config-name> <rule> [args..]
        Run with configuration template
        Runs a rule using a predefined configuration template.
        Usage: podman_run_with_config <config-name> <rule> [args..]

      podman_run_parallel <image> <rule> [--parallel <count>] [args..]
        Run rule in parallel containers
        Runs the same rule in multiple parallel containers.
        Usage: podman_run_parallel <image> <rule> [--parallel <count>] [args..]

      podman_run_limited <image> <rule> [--cpus <n>] [--memory <size>] [args..]
        Run with resource limits
        Runs a rule with CPU and memory limits.
        Usage: podman_run_limited <image> <rule> [--cpus <n>] [--memory <size>] [args..]
        Example: podman_run_limited myimage:latest build --cpus 2 --memory 4g

      podman_run_background <image> <rule> [args..]
        Schedule background container execution via memodb
        Schedules a container execution in the background using memodb.
        Usage: podman_run_background <image> <rule> [args..]
        Requires memodb module to be loaded.

      podman_run_background_wait <image> <rule>
        Wait for background container execution to complete
        Waits for a background container execution to complete.
        Usage: podman_run_background_wait <image> <rule>

  podman_rules - Podman support rules for Bar
    
    Integrates podman container support with Bar's standard build system.
    Provides hooks for container-based builds and tests.

    Rules:

      is_podman_installed
        Check if podman is available on this system

      podman_check_deps
        Verify podman is properly configured and ready to use

      podman_setup_qemu
        Setup QEMU for multi-architecture support

      podman_list_archs
        List all available architectures

      podman_list_networks
        List all podman networks

  rad - Support for the radicle p2p git frontend

    Functions:

      rad_import import a git repository into radicle

  radicle - managing a radicle installation itself

    Functions:

      radicle_update [HEAD|TAG]
        Updates the users radicle installation to [HEAD|TAG] or the latest stable

      update_radicle_desktop [HEAD|TAG]
        Build radicle-desktop, [HEAD|TAG] or the latest stable

  release - Rules for software releases
    
    The 'release' module implements a (opinionated) release workflow. Making releases starts
    on a git toplevel in a development or main branch, a devel branch does not need to be
    clean.  For each release a new branch/worktree based on the last commit is created. All
    release work will be done in that branch/worktree. This can be a interactive process, when
    release_tests fails one can correct things and commit them until the tests pass. The
    actual release procedure then evaluates 'release_prepare', 'release_commit', 'release_tag'
    and 'release_publish'.  Finally 'release_postprocess' and 'release_cleanup' are evaluated.
    
    Any non published release can be aborted/deleted.
    
    This only defines the bare workflow the actual implementation hooks into the
    'release_rules'.
    
    We still make a few fixed assumptions:
    - Versioning is based on semver
    - Releases are called 'release'
    - The main branch can be named 'main' or 'master'
    - Development branches matching 'devel' 'devel-*' or 'devel/*'
    
    The worktree tracks its state in a local git config bar.release.state:
    - start:
      The worktree is created but not tested yet.
    - tested:
      All test passed.
    - changelog:
      generated the changelog and committed it.
    - ready:
      The worktree/branch is ready for publishing
    - published:
      Uploads succeeded.
    - done:
      Postprocessing done.
    - finished:
      Final state, everything finished.
    
    Further following git config variables are set:
    - bar.release.startbranch:
      Branch this release initiated from (usually main or devel).
    - bar.release.version:
      The (semver) version of this release.
    
    Release Configuration and Workflow
    
    Every release starts in its own git worktree. All testing, and fixes necessary for the
    release are made in that worktree.
    
    Configuration of release workflows is done by extending the rules from 'release_rules'.
    Many other '*_rules' modules already hook into these rules and do the 'right thing'.
    The parts that can configured in Barf are:
    - release_prepare:
      Any extra preparation for releases. 'cargo_rules' already bumps the version.
    - release_tests:
      Whatever tests are necessary for making a software release. 'lints' and 'tests' are
      already defined dependencies. Anything more needs to be added in Barf.
    - release_preprocess:
      Any final work that needs to be done before publishing. This should not change the
      project files anymore. This is the place to do git tags, merge/fast-forward branches
      or switch over to other worktrees. For convenience the 'release_as_*' rules exist to
      do this work.
    - release_publish:
      The actual publishing part. May need access to the internet.
      The 'git_rules' and 'cargo_rules' already hook into this.
    - release_postprocess:
      Any work to be done after the publishing. Like rebase work/devel branches on the
      release and so on. For example resetting the main branch and rebase the branch from
      where the release started:
       rule release_postprocess: release_reset_mainbranch release_rebase_startbranch
    - release_cleanup:
      Final clean up work to remove artifacts not needed after the release.
      May just use 'rule release_cleanup: clean' when build artifacts can be deleted.

    Functions:

      release_is_patch checks whenever the current worktrees release is a patch release

      release_state_matches [[!]statematch]
        Check if the current state matches (does not match with '!').

      release_into <target_version>
        switch over to another existing release worktree, reset it to the current release state
        We prepare releases in dedicated worktrees. This would leave a lot worktrees behind.
        This provides the facility to coalesce worktrees to keep only major, majorpre1x, or majorminor versions.

      release_into_major Transfer over to major version, for workflows that keep 'release-x' worktrees and branches

      release_into_majorpre1x Transfer over to major version with 0.x major semantic, for workflows that keep 'release-0.x' and 'release-x' worktrees and branches

      release_into_majorminor Transfer over to major.minor version, for workflows that keep 'release-x.y' worktrees and branches

      release_rename <version>
        rename worktree and cd into that

      release_rename_major Rename worktree to'release-x' with major version only.

      release_rename_majorpre1x Rename worktree to'release-x[.y]' with 0.x major semantic.

      release_rename_majorminor Rename worktree to 'release-x.y' with major.minor.

      release_fastforward_branch <branch>
        fast-forward <branch> to the release branch if possible

      release_reset_mainbranch Find main branch (main/master) and reset it to the release branch

      release_rebase_branch <branch>
        rebase <branch> onto the release branch

      release_rebase_startbranch Rebase the branch that started the release on the release branch.

    Rules:

      release
        Makes a software release. Takes an optional version number starting with a digit or a
        name for the generating the version by 'release_generate_version_hook' as argument.
        When no argument is provided it defaults to 'auto'.

      release_as_major
        To be used from release-preprocess. Creates or resets the release worktree to release.x major.

      release_as_majorpre1x
        To be used from release-preprocess. Creates or resets the release worktree to release.x[.y] major with 0.x major semantic.

      release_as_majorminor
        To be used from release-preprocess. Creates or resets the release worktree to release.x.y major.minor.

  release_rules - The rules called by the release rule

    Rules:

      release_prepare
        Update manifest, bump versions, generate changelog an other versioned artifacts etc..
        Should leave the tree in a uncommited state. Note that 'build_versioned_artifacts' is
        already a dependency here.

      release_tests
        All tests and lints to run for a software release.

      release_changelog
        Create the changelog and commit it, may include tags etc.

      release_preprocess
        Local actions to be done before the release, rebase main/devel on it etc.

      release_publish
        Deploy the release, upload it to servers, git push or whatever necessary. Since network
        operations can be fallible this can be restarted.  Should start with them least critical
        (git push) and finally publish it to registries.

      release_postprocess
        Local actions to be done after the release.

      release_cleanup
        Final cleanup for the release, clean/delete worktrees etc

      release_generate_version
        Canonicalize version names. Queried when the the version to 'release' does not start with a
        digit or was not given. Other modules should plug in here. Names are 'auto', 'major',
        'minor', 'patch'. Custom rules may support more names such as 'prerelease', 'alpha', 'beta'
        and so on. 'auto' is the default for 'release'.

      release_current_version
        Query the current version of this project.

  rule_lib - The library that implements rules, always loaded

    Variables:

      BAR_KEEP_GOING (gx)
        Controls keep-going mode: 0 or 1 stops at first failure (default), numbers >= 2 allow that many failures before stopping, non-numeric values (yes/true/etc) continue until exhaustion. In keep-going mode, pure clauses continue executing after failures though failures are still propagated. Non-pure or conclusive clauses halt keep-going within the current rule.
        Default: ${BAR_KEEP_GOING:-1}

    Functions:
        See RULE DEFINITION SYNTAX in help for details

      newrule <name:> ...
        Creates a new rule, deletes the existing one.
        Used to replace already existing rules/clauses.

      rule_delete <name>
        Deletes a rule (and all it clauses).
        Can only be used when the rule was not evaluated yet.

      rule_rename <oldname> <newname>
        renames a rule
        Can only be used when the rule was not evaluated yet.

      clause_local [var[=value]..]
        Makes scalar variables local to a clause.

      rule_eval <name> [args..]
        Evaluates a rule
        Normally this is used by bar itself but it can be used from function and rule bodies
        to evaluate rules.

      rule_exists <name>
        checks if a rule is defined

      rule_list [pattern]
        lists all loaded rules or rules matching pattern
        Because of automatic module loading this list is only a current view for diagnostics.

  run - Module for running tests in resource limited environments

    Variables:

      NICE_LEVEL (gx)
        The nice level 'run_test' uses
        Default: ${NICE_LEVEL:-1}

      TIMEOUT_SECS (gx)
        Timeout for tests
        Default: ${TIMEOUT_SECS:-300}

      MEMORY_KB (gx)
        Memory limit for tests
        Default: ${MEMORY_KB:-33554432}

  search - Rules to search text in a project

    Rules:

      search_for [rg_args..]
        General searching for text, pretty paging the results.

      search_issues
        Searches for 'FIXME:|TODO:|PLANNED:' with some context around.

      search_fixmes
        Searches for 'FIXME:'.

      search_todos
        Searches for 'TODO:'.

      search_planned
        Searches 'PLANNED:'.

  semver_lib - Parsing and manipulating (simplified) semantic versioning strings
    
    For the sake of sanity not all possible semver syntax variants are
    handled. Esp. manipulation is only implemented for the common cases. Patches welcome.

    Functions:

      semver_parse <semver> [major [minor [patch [prerelease [build]]]]]
        parses semver into the provided variables

      semver_validate <version>
        Validate if string is a valid semver

      semver_shortversion <version>
        shortens a semver to the first non zero part 0.2.1-pre0+123 -> 0.2

      semver_major <version>
        shortens a semver to major

      semver_majorpre1x <version>
        shortens a semver to major with 0.x major semantic

      semver_majorminor <version>
        shortens a semver to major.minor

      semver_is_patch <version>

      semver_increment <version> <release|major|majorpre1x|minor|patch>
        Increment selected part
        'release' strips any 'prerelease' part
        'majorpre1x' increments the minor part on a 0.x version

      semver_cmp <a> <b>
        comparing 'a' with 'b', prints 'lt', 'eq' or 'gt'

      semver_lt <a> <b>
        succeeds if a < b

      semver_le <a> <b>
        succeeds if a <= b

      semver_gt <a> <b>
        succeeds if a > b

      semver_ge <a> <b>
        succeeds if a >= b

      semver_eq <a> <b>
        succeeds if a == b

  shellcheck - Support for the shellcheck shell linter
    
    Shellcheck is surprisingly slow.  In projects using bar one can prevent the bar files
    themselves being checked by setting in 'Barf':
    
     SHELLCHECK_LS_FILES=":!bar :!Bar.d"  # Perhaps adding ':!Barf' too

    Variables:

      SHELLCHECK_LS_FILES (gx)
        List of file match rules for git-ls-files.
        Default: ${SHELLCHECK_LS_FILES:-}

    Functions:

      shellcheck_is_shscript <file>
        Check if <file> is a shell script

      shellcheck_list_shscripts List all shell scripts under git control

      shellcheck_lint Test if all shell scripts under git control pass the linter.

    Rules:

      shellcheck_has_shscripts
        Checks if the project has any shell scripts. Must be called from a git toplevel.
        Checks whenever any shell scripts are versioned in gitq

  shellcheck_rules - Hooks shellcheck_lint into the std lint rule

    Rules:

      is_shellcheck_available
        Checks if shellcheck is installed and the project has any shell scripts.

  ssh - Access remote hosts

    Functions:

      ssh_ping [[user@]host]
        Checks if one can connect to a remote host.

      ssh_cmd [ssh_args]..
        Executes a ssh command.

      ssh_bg_cmd [ssh_args]..
        Executes a ssh command in the background.

  std_lib - Library of general functions which are always present

    Functions:

      memo [-c|-d] [cmd args..]..
        memoize the result/stdout/stderr of commands, will always return the same result again
        Memoizes for the current run only. See memodb for persistent memoization.
        Memoized functions must be pure and only depend on their arguments and the current working directory.
        Can not handle null bytes in stdout/stderr, use memodb when that is required.

      memofn <functionnames..>
        rewrites a function into a function that uses 'memo'
        This renames the given functions to 'nomemo_<functionname>' and creates a wrapper
        function with the original name that memoizes the result.

      is_scalar <name>
        Checks that a variable 'name' is a scalar (non array) variable.

      called_as [pattern..]
        Checks whenever bar is called as a symlink matching the given patterns.
        This is usable as a rule dependency.

      is_command_installed <command>
        Checks whenever a function exists or a shell command is in PATH.

      bar_now Returns the current timestamp in microseconds since epoch.

      hash_args <arguments..>
        returns the sha1hash of all supplied arguments

      iset <target> [values]..
        indirect assign values to a variable given by name, assigns to an array when more values are given.
        When target is empty or '[*]' (missing name) then this is a no-op. Otherwise target
        must be a declared variable When no values are given then target is cleared.

  std_rules - Standard rule targets where other modules can hook in
    
    Here we define targets which can be universally used. Other modules will hook their more
    specific rules into these. Usually projects support only a subset of these standard targets.
    This is fine. Any not hooked rule will only print a debug message and being a no-op otherwise.
    
    Undesired std_rules can be renamed or deleted in the rulefile, this is preferred to
    editing the std_rules file because it allows for easier updates of the std_rules file. All
    these std rules just structure the targets and give debug output on the leafs. Actual
    actions are hooked in by other modules.

    Rules:

      all
        Rule that builds the software, documentation and necessary artifacts but will not do any testing.

      lints
        Run all linters.

      build
        Build libraries and executables, but not tests, docs and other non mandatory things.

      tests
        Run tests, first unit tests then integration tests.

      doc
        Build the documentation. When supported the documentation is linted as well.

      bench
        Build and run benchmarks.

      fmt
        Reformat the source code in place.

      fix
        Apply trivial fixes, should be non breaking changes.

      update
        Update dependencies.

      run
        Run the main binary if any. This already depends on 'build' but the actual run action has
        to be hooked into 'run' by the user in the rulefile.

      audit
        Audit for security issues.

      lint_sources
        Run a static analyzer over the source code.

      lint_docs
        Checks the documentation.

      fetch_resources
        Whenever resources are externally managed and not part of the versioned source tree they
        should be added to this rule.

      build_assets
        Whenever assets that are not part of the normal build process have to be build, this should
        be added to this rule.

      build_libs
        Build libraries.

      build_bins
        Build executables.

      build_tests
        Build all tests

      build_unit_tests
        Build unit tests.

      build_integration_tests
        Build integration tests.

      build_docs
        Build the documentation

      build_benches
        Build benchmarks.

      build_examples
        Build examples.

      build_versioned_artifacts
        Usually versioned files should be source and not generated by any rule. Sometimes one wants
        exceptions from this rule. Like generating a README or CHANGELOG. This can be hooked into
        this rule. This can be run before committing or as part of the commit check where one checks
        for git_is_clean after running generate_versioned. When this failed then the user forgotten
        to update the versioned files.

      test_units
        Run unit tests.

      test_integrations
        Run integration tests.

      test_expensive
        Some tests are very expensive. Since building and running makes no much difference then
        this is accumulated in this single rule. This allows to exclude expensive tests from the
        normal workflow rules and include them on demand.

      testrelease
        Runs the 'release_tests' in a testdir. Uses the current index (staged or last commit) This
        is not a full release test as it wont attempt to bump version and does not generate
        changelogs etc.

      benchmark
        Running benchmarks. This does not include building benchmarks, see the 'bench' rule.

      deploy
        Deploy the project to some server or similar.

      clean
        Clean all build artifacts, but keep assets and user configuration.

      force_reset
        Danger-Zone: reset the project into a pristine state with all non versioned data deleted or
        reset to the original state.

      activate
        Activates maintainer curated githooks and other setup to use bar in a checked out
        repository.

      --help
        Show help.

      -h
        Show help.

      --debug [rule] [args]
        Evaluate a rule with at debug level.

      --trace [rule] [args]
        Evaluate a rule with at trace level.

      --verbose [rule] [args]
        Evaluate a rule at increased verbosity level.

      --quiet [rule] [args]
        Evaluate a rule with supressed output.

  testdir - Creating an isolated directory for testing
    
    Unlike other frameworks which do 'pre-commit' processing bar will not modify, block or
    stash the users workdir. Instead it creates and manages directories which become populated
    with the projects files. These testdirs also act as caches for build artifacts an can be
    seeded from previous runs to increase build/test speed. Testdirs are automatically garbage
    collected, only a few most recent used ones are kept for inspection and seeding new build.
    
    Testdirs are created from git 'treeish' objects. Thus git is mandatory.
    Testdirs are created in the git toplevel and have the name constructed from
    '$TESTDIR_PREFIX-<timestamp>-<treehash>/'

    Variables:

      TESTDIR_PREFIX (gx)
        Prefix used for testdirs.
        Default: ${TESTDIR_PREFIX:-.test}

      TESTDIR_KEEP (gx)
        How many old testdirs.
        Default: ${TESTDIR_KEEP:-5}

      TESTDIR (gx)
        Path to the current testdir.

      TESTDIR_PREV (gx)
        Path to the previous testdir if exits.

    Functions:

      is_testdir_used Succeeds when a testdir is in use and current working directory is inside it.

      testdir_gc Cleanup old testdirs.
        Removed old unused testdirs. This is called automatically. It is only necessary to
        manually call this when testdirs piled up because they where in use or when
        'TESTDIR_KEEP' was decremented.

      testdir_clean Cleanup all testdirs.
        Removes all unused testdirs.

      testdir_list Lists all testdirs.

    Rules:

      testdir_enter [treeish]
        Sets up a test directory change dir into it.
        Must be called from the git toplevel. Takes an optional treeish to construct the testdir
        from. When no treeish is given then current index is used.  After creation the
        'testdir_enter_hook' rule is evaluated. This rule is used to prep/populate/seed the
        testdir. When a previous testdir exists then 'TESTDIR_PREV' point to it, the hook can use
        that as source for populating already build artifacts.

  tty_lib - Module for terminal control codes
    
    These codes are stored in the 'TTYCTL' associative array.  The 'TTYNIL' mirrors the keys
    from 'TTYCTL' but maps them to empty strings.  Finally 'TTYOUT' and 'TTYERR' refer to
    'TTYCTL' when the associated output stream is a non dumb terminals and refer to 'TTYNIL'
    when it is not a terminal or a dumb terminal.
    
    The keys in 'TTYCTL/TTYNIL' are as following.
    
    Two character control sequences:
     'ce' - Clears from cursor to the end of the line.
     'cl' - Puts the cursor in column 1 and clears the line.
     'cs' - Clears the entire screen.
     'cr' - Puts the cursor in column 1.
    
    Three character styles:
      The first two characters set the foreground and background color:
       '_' - no change.
       'k' - black
       'r' - red.
       'g' - green
       'b' - blue.
       'y' - yellow.
       'm' - magenta.
       'c' - cyan.
       'K' - bright black
       'R' - bright red.
       'G' - bright green
       'B' - bright blue.
       'Y' - bright yellow.
       'M' - bright magenta.
       'C' - bright cyan.
      The third character set the style uppercase enables, lowercase disables:
       '_' - no change.
       'B' - bold.
       'D' - dim.
       'U' - underline.
       'I' - italic.
       'R' - reverse.
       'S' - strike-through.
       'n' - normal (has no uppercase).
    
    Finally there is a the one character 'n' code that resets the style completely.
    
    For an example to see how this is used look at the 'std_lib' module.

    Functions:

      tty_echo [echo_args]
        Echos only when the output is a tty.

  watch_rules - Continious supervision by a file watcher

    Rules:

      watch [rule [arguments..]]
        Evaluates a rule or MAIN whenever a versioned files changed in the current directory.
        File watchers hook into this rule.

      watch_git_index [rule [arguments..]]
        Evaluates a rule or MAIN whenever a the git index changed.
        File watchers hook into this rule.


INITIAL INSTALLATION

  Bar can be invoked in different ways:

  1.  Installed in $PATH:
      To make this work it is best to clone bar locally and symlink the checked out files
      to your '.local/' tree. This allows easy upgrades via git:

      When you have radicle (https://radicle.xyz/) installed you can clone it with:

       rad clone rad:z3WhBdHt1VVNyeQ61zpY66pNzuSrP

      Otherwise it can be cloned by git with:

       git clone https://seed.pipapo.org/z3WhBdHt1VVNyeQ61zpY66pNzuSrP.git bar

      Then you can install it in your ~/.local tree with:

       cd bar
       ./bar init_install ~/.local/

      This creates symlinks to 'bar', 'please', 'Bar.d', 'Barf.default', 'Please.default' in ~/.local
      and symlinks 'Bar.d/*' to '~/.config/please/'.

      Now 'bar' is installed, check it with 'bar help'

  2.  The local './bar' initialized from above:
      Since bar is a bash script, it’s easy to version and ship it with other software.
      To initialize it in a current directory use either of:

       bar init          # creates bar, Barf, Bar.d/
       bar init --hidden # creates .bar, .Barf, .Bar.d/

      After that Barf can be customized, unnecessary modules in Bar.d/ may be deleted.
      Once that is done these files should be put under version control.

      When the installed bar from 1. becomes updated then a local version can be updated by:

       bar init --update

      This only updates 'bar' and any existing modules in 'Bar.d/' but will not touch the
      'Barf' file since that is meant for local customization.

       bar init_update_merge_barf

      Will do a merge of the installed 'Barf' file with the local one. This *will* leave 3-way
      conflict markers behind when there are changes. These must be manually resolved!

      The updated files should then be committed to version control.

  3.  githooks symlinked from './.git/hooks/*' -> '../../bar'
      'bar' has support to be used as githooks. This needs manual enabling. Individual hooks can be
      enabled or disabled with 'bar githook_enable <hook>' and 'bar githook_disable <hook>'.
      For existing projects using 'bar' a maintainer can setup rules to activate all necessary hooks.
      This then can be activated by './bar activate'.


GIT HOOK ACTIVATION

  Bar will be inert in a project that ships with bar initialized. One can call it manually by
  './bar'.  A maintainer of the project may define 'activate' rules that activate
  githooks. These are then activated by './bar activate'. This will create symlinks in the
  '.git/hooks/' directory to the 'bar' script. These hooks will then be executed when the
  respective git hook is triggered. The rules for these hooks are defined in the 'Barf' file
  in the project directory.


RULE SEMANTICS

  Rules are a named, ordered collection of clauses. They are considered pure with inputs being
  the current directory name and the rule's arguments. Note that the content of files is *not*
  considered and the outcome of the rule should not depend on external variables. Rules are
  evaluated lazily, only once; the result of a rule is cached. This means that rules, esp. the
  actions within the body must have deterministic sematics. For performance reasons this is
  not enforced but one may observe surprising behavior when this requirement is violated.

  Rules will be autoloaded on demand from modules in 'Bar.d/' (see below AUTOMATIC MODULE
  LOADING). For rules where this fails a fallback exits that creates an implicit rule for any
  command and shell function defined with a body calling the respective command.

  Each clause in a rule has its own set of dependencies. Dependencies are other rules which
  are evaluated in order. When a (normal) dependency fails then rule it is considered to be
  failed as well and no further attempts to evaluate following dependencies and clauses are
  made.

  Dependencies can be used as 'checking' dependency which either expects the dependency to
  succeed (suffixed with a '?') or fail (prefixed with a '!'). When such a checking dependency
  fails then the rest of the current clause is skipped. This allows conditional evaluation.
  Further dependencies may be tagged as unconditional by suffix them when a tilde '~', these
  are just evaluated but rules evaluation will proceed unconditionally even if such a
  dependency fails.

  Finally every clause can have an optional body. This are shell commands executed when all
  dependencies succeeded. Because of quotiung in shell becomes a but unwieldly there are
  shortcuts to make the body refer to shell functions, this should be preferred.


RULE DEFINITION SYNTAX

   rule [[--rule-flag].. <name>:] [--clause-flag].. [[!]deps[?|~][??] args..]..
   rule [[--rule-flag].. <name>:] [--clause-flag].. [[!]deps[?|~][??] args..].. -
   rule [[--rule-flag].. <name>:] [--clause-flag].. [[!]deps[?|~][??] args..].. -- [body..]

  * [--rule-flag]

    Sets flags for a rule. Flags are additive and affect the rule, not single clauses.

    --always
      The result is not cached and the rule will be always evaluated again.

    --bare
      When this rule is called as initial rule (bar <rule>) then do not eval SETUP,
      PREPROCESS, POSTPROCESS and CLEANUP.

    --reverse
      The clauses of this rule will be evaluated in reverse order

    --disjunct
      Makes the clauses of this rule disjunctive. The rule will succeed with the first
      successful clause. Normally clauses are conjunctive and fail with the first failing
      clause.

    --meta
      Metarules do not have their own clause local variable. This allow them to modify
      the clause locals of the calling rule. 'clause_local' for example is a metarule.
      User defined rules that want to modify clause locals will need to be meta too.
      See 'try_cargo_toolchain' for an example.

  * [<name>:]

    A rule can have a optional name (suffixed with a colon).  When the name is not given it
    defaults to 'MAIN'.  Rules that have only a name but no dependencies and no body create a
    body that calls a bash command or function with the same name as the rule passing rule
    arguments to it.

  * [--clause-flag]..

    Sets flags affecting this clause only.

    --conclusive
      Makes a clause result conclusive, if not skipped no more clauses will be tried.

    --default
      Default clauses will be kept at the end of the list of clauses. Using '--default'
      together with 'disjunct' rules allows one to hook up action while maintaining a fallback
      or warning/error when no hooked up rule succeeded.

  * [[!]deps[?|~][??] args..]..

    Dependencies, are a list of other rules that this rule depends on. These will be executed
    in order. If any of the dependencies fails then this rule is considered failed as well, no
    further dependencies, bodies or clauses are executed unless this dependency is a checking
    or unconditional one.

    A dependency can be either prefixed with an exclamation mark or suffixed with question
    marks or a tilde. The exclamation mark prefix expects the dependency to fail and will
    continue then, if the dependency succeeds then the remaining dependencies and the rule
    body are skipped. With a question mark as suffix the dependency is expected to succeed,
    when it fails then the rest of the dependencies and the rule body is skipped. With a tilde
    as suffix outcome of the dependencny is ignored and the rest of the dependencies and the
    rule body is executed.  The difference here is that these checking dependencies decide if
    a clause should be skipped or succeed while a failure on a normal dependency will fail a
    rule instantly. When a rule is suffixed with two question marks then this behaves like
    "'rule_exists? dep' dep". Making this dependency optional. This can be combined with the
    another question mark, a tilde, or explamation mark prefix. This is commonly used to
    define optional rule hooks.

    Dependencies can have arguments. When provided then the dependency and its arguments must
    be quoted. These arguments are passed in the 'RULE_ARGS' array to the body of the rule.

  * -

    When there is a single hypen '-' at the end of a rule definition then the rule body is a
    call to a command or function of the same name as the rule with all arguments passed. This
    is used when one wants to add dependencies to a existing command or function.

  * -- [body..]

    After a double hyphen a optional rule body follows. This are the commands to
    execute for this clause. If these fail then the rule is considered failed. When a name but
    no body and no dependencies are given then the body defaults to the rule name. This makes
    it easy to translate simple parameterless commands and functions to rules. Usually the
    body has to be quoted to prevent shell expansions at definition time. Later when a rule
    becomes evaluated the body is passed to 'eval'.

    When the body is omitted, then the rule body is read from stdin. This allows to use
    heredocs or herestrings or read the body from a file.

    When a rule overrides an existing command a 'NOTE' will be printed on higher verbosity
    levels. The user is responsible for sensible overiding of commands.

  Rules must not have mutually recursive dependencies. When such is detected the
  evaluation aborts.

  Examples:

   # Add a clause to 'MAIN'
   rule -- echo hello

   # Different ways to define rule bodies
   rule foo_ok: -- echo inline
   rule foo_ok: -- '
       echo inline quoted
   '
   rule foo_ok: -- <<<"echo herestring"
   rule foo_ok: -- <<EOR
      echo heredoc
   EOR

   # Make a rule that fails
   rule foo_fail: -- false

   # functions and commands can be used as rules, this should be preferred
   function example
   {
       echo "i am example called with $*"
   }

   # add a tests rule with 3 clauses
   rule tests: foo_ok? 'example "argument"' -- echo "foo_ok success"
   rule tests: !foo_ok -- echo "This is never called, but MAIN still passes"
   rule tests: foo_fail? -- echo "This is also never called"

   # add tests to MAIN
   rule tests

  For some more examples see the 'example' file that ships with bar.


  Special Rules and Names

    Rules that do not have a name fall back to 'MAIN', when no rule name is given at execution
    time then 'MAIN' will be evaluated.

    If exist 'SETUP', 'PREPROCESS', 'POSTPROCESS' and 'CLEANUP' will be called.
    * SETUP
      Should set up the environment for all subsequent evaluation. Rules in 'SETUP'
      should be self-sufficient and not expect a working environment yet.
      A failure here will abort bar.
    * PREPROCESS
      Doing any preparatory tasks within the set up environment.
      A failure here will abort bar.
    * MAIN
      When the bar is not called via a symlink or the user did not provide a rule to
      evaluate, then MAIN is the default rule. Otherwise the user supplied or symlink-deduced
      rulename will be evaluated. The result of this rule determines the exitcode of bar.
    * POSTPROCESS
      Is for evaluating things after the main rule finished. The environment and
      state is still the same as in the main rule. A failure here will be reported but not
      reflected in the exitode.
    * CLEANUP
      Is for removing temporary resources and doing any other kind of cleanup work. This
      means the state and environment at cleanup time may be differen/partially destructed and
      should not be relied upon. Clauses in the CLEANUP rule are evaluated in reverse order of
      their definition. A failure here will be reported but not reflected in the exitcode.

    After a rule that is not flagged with '--always' got evaluated it is not allowed to add
    more clauses to it anymore, as the outcome would be unexpected and impure.


  Keep-Going Mode

    Bar supports a keep-going facility via the BAR_KEEP_GOING environment variable. When
    enabled, clauses that fail do not immediately halt rule evaluation—subsequent clauses
    continue to execute, though the overall failure is still propagated.

    BAR_KEEP_GOING can be set to a number which is decremented on each failure, when this
    reachs zero no more failures are accepted and the evaluation will terminate.

    When BAR_KEEP_GOING set to anything else than a number then evaluation will accept
    infinite failures.

    When not set then it defaults to one then evaluation will terminate on the
    first failure.

    Clauses marked '--conclusive' immediately halt keep-going.


  Purity Requirements

    All rules and functions must be sequentiually pure in their success branch. This means
    side effects must not alter the behavior of subsequent rules when they fail. For rules
    where this may be the case, like rules changing directories they must rather die than
    proceed with a undefined state in case of a failure.


RULE ENVIRONMENT

  Rules implicitly depend on the arguments passed and the current directory they run in. Any
  other state or environment variable is not considered. When the result of a rule would
  depend on such external state then the rule is impure and may lead to wrong
  results.


MODULES

  Modules are shell snippets normally located in '$BAR_DIR' ('Bar.d/', 'bar.d/', '.Bar.d/' or
  '.bar.d/') when called as 'bar' or symlink or '~/.config/please' when called as 'please'.
  The 'std_lib' and 'rules_lib' are always loaded at startup. Modules matching '*_rules' are
  also autoloaded at startup. Any other '*_lib' must be manuallly loaded with 'require'.
  Modules with simple alphabetic names without underscores are lazy loaded on demand.

  Modules are loaded with the 'require' function. This ensures that they are loaded only once.
  Require will load modules outside of '$BAR_DIR' when a path containing at least one slash
  is given.

  The first variant for modules that are initially loaded is used for modules that define
  helper functions that don't have associated rules or rules that are primarly used for
  checking conditions before delegating to the actual rules which do the work.

  The second variant is for mudules that define rules which are self-contained and can't
  extend existing rules with new clauses. Here are the rules and functions which do the actual
  work.

  The reason for this is that some will add clauses to existing rule which must be done before
  the rules are evaluated while we can improve performance by lazy loading modules that are
  not always needed.

  The module loader derives the module name from the rule name by removing any prefixes and
  suffixes. Thus the rules which shall trigger loading must follow the naming schema with the
  module name after the prefixes in the rule name.

  Per convention '<name>_rules' is a module that contains the rules (and few functions) that
  that add clauses to other rules to make 'name' accessible. These '*_rules' files are
  autoloaded at startup.

  '<name>_lib' are libraries for support functions other modules may use (with 'require
  name_lib').


AUTOMATIC MODULE LOADING

  Rule names may include underscores, then the word before the first underscore is used to
  determine a module name used for auto-loading rules. They can have special prefixes which
  are removed when auto-loading:

  - 'is_', 'has_' and 'try_' are reserved for check rules they don't have any special
    semantic except for being stripped at auto loading.

  When a rule is not found then a module name is derived from the rule name by removing the
  'try_', 'is_', 'has_' prefixes and cutting off anything behind the first
  underscore. Eg. 'try_module_check' results in 'module'. This is then searched as
  'Bar.d/module', 'bar.d/module', '.Bar.d/module' or '.bar.d/module' (or respective please
  variants) and loaded if present. Thus modules that are automatically loaded must be named by
  single word without underscores.


STANDARD RULES

  Bar ships with a module that defines a set of standard rules where other modules can add
  clauses to. Check 'Bar.d/std_rules' for details.

  'SETUP', 'PREPROCESS', 'MAIN', 'POSTPROCESS' and 'CLEANUP' are special rule names called
  approbiately.

  The 'clause_local [var[=value]]' is for defining variables that are local to a clause.  Only
  scalars can be clause local. Ideally this should be used as dependency and setting a a
  variable to a value. It can be used in a function as well esp when creating other metarules.


MAIN API

  bar - The main BAsh Rulez script
    
    Provides only basic functionality for module loading and debugging and starting rule
    evaluation. Loads the 'std_lib' and 'rule_lib' to make them omnipresent.

    Functions:

      bar_main [rulefile] [--bare] [rule [arguments..]]
        Loads a rulefile and executes rule with optional arguments
        [rulefile]           - File to load the user rules from.
                               Default: Barf|barf|.Barf|.barf when called as bar or symlink.
                                        Pleasef|pleasef|.Pleasef|.pleasef|\$HOME/.Pleasef when called as please.
        [--bare]             - Don't eval SETUP, PREPROCESS, POSTPROCESS and CLEANUP.
        [rule [arguments..]] - Target rule with arguments.
                               Default: MAIN

      require [--opt] [modules..]
        Loads modules.
        [--opt]     - Make the modules optional, when they do not exist then no warning is emitted
                      and no failure returned.
        [modules..] - List of module names to be loaded. Modules are loaded only once, further
                      attempts to load it will be a no-op.

      DBG [message..]
        For print-style debugging, should not be present in production code.

      die [message..]
        Prints 'message' to stderr and exits with failure
        Unexpected fatal problem. Will terminate execution, CLEANUP rules will be called.

      error [message..]
        Print a error message to stderr.
        Unexpected but not fatal.

      warn [message..]
        Print an warning to stderr.
        Expected, non fatal problem.

      note [message..]
        Print an important notice to stderr.
        Important message the user should be notified about.

      info [message..]
        Print an informal message to stderr.
        Broader message about non rule related progress.

      debug [message..]
        Print a debug message to stderr.
        Finer grained progress messages.

      trace [message]
        Prints a trace message to stderr.
        Very verbose function call or finer messages.


DOCUMENTATION WRITING GUIDELINES

  Bar uses a structured documentation format that serves both human readers and automated
  tools like help generation and bash completion. Documentation uses '##' comment markers.

  File-Level Documentation:

    Use '###' at the start of a file to describe the module/file:

     ### This module provides cargo/rust support.

  Variable Documentation:

    Variables declared with 'declare' can be documented on the same line:

     declare -g MYVAR="value" ## Description of the variable

  Function Documentation:

    Functions should have documentation in the form:

     function name ## [--opt] <required> [optional..] - Short description

    The parameter list uses a formal syntax:
    - <param>    - Mandatory parameter (prototype)
    - [param]    - Optional parameter (prototype)
    - <param..>  - One or more occurrences (suffix .. after closing >)
    - <param>..  - One or more occurrences (suffix .. after closing >)
    - [param..]  - Zero or more occurrences (suffix .. after closing ])
    - [param]..  - Zero or more occurrences (suffix .. after closing ])
    - <> and []  - Can be nested
    - word       - Anything not in <> or [] is a literal (like 'as', 'into', ':')
    - --flag     - Literal flag (can be in <--flag> for prototype completion)
    - -f         - Literal short flag
    - a|b        - Alternatives where a and b can be any of the above
                   (a|b|c|d etc. any number allowed)
                   Note: <a..|b> is at least one 'a' or a single 'b'
                         <a|b>.. is at least one of 'a' or 'b'
    - param      - Parameter names serve as prototypes for completion

    Examples:

    - function rename ## <this> as|into <that> - rename this as/into that
      Requires literal 'as' or 'into' at the 2nd position.
    - function process ## <input:> <output> - process input to output
      The colon ':' after <input> is a literal character.

    Parameter identifiers start with an alphabetic character followed by [[:alnum:]-_].
    These serve as prototypes for completion and are refined in following documentation.

    Multi-line documentation continues on following lines starting with '##':

     function foo ## [--verbose|-v] <input> [output] - Process files
     {
         ## [--verbose|-v] - Enable verbose output
         ## <input>         - Input file to process
         ## [output]        - Optional output file (defaults to stdout)
         ...
     }

  Rule Documentation:

    Rules are documented with '##' lines before the rule definition:

     ## <target> - Build the specified target
     rule build:

    For rules with parameters, use the same formal syntax as functions:

     ## [--toolchain] <target> [options..] - Build target
     rule build:

  Documentation Prototypes for Completion:

    Parameter identifiers map to completion functions. Common prototypes include:

    - <file>      - Any file on filesystem
    - <directory> - Any directory
    - <path>      - Any valid path
    - <text>      - Any text input
    - <number>    - Numeric input
    - <rule>      - Existing rule name
    - <command>   - Command or function name

    Module-specific prototypes can be defined (e.g., <toolchain> in cargo module).
    See contrib/bar_complete for completion implementation details.
    
  Defining Custom Completion Prototypes:

    Modules can define custom prototypes for parameter completion using single-hash comments
    that must start at column 0 (no leading whitespace):
    
      # prototype: "mytype" = "file"
      # prototype: "myfile" = "file existing local"
      # prototype: "mycommand" = "command"
      # prototype: "toolchain" = "extcomp cargo"
      # prototype: "gitargs" = "extcomp git"
    
    These definitions register a prototype with a completer. The format is:

      # prototype: "prototype_name" = "completer_spec"
    
    Where completer_spec is either:
    - A built-in completer name (file, directory, path, rule, command, etc.)
    - A completer with predicates (e.g., "file existing local")
    - "ext function_name" for external/custom completers (calls bar --bare function_name)
    - "extcomp command" for black-box external command completion (invokes command's native bash completion)
    
    The "extcomp" type enables leveraging any command's existing bash completion by invoking
    it as a black box. For example, "extcomp git" will use git's native completion for all
    git subcommands and flags. This works with git, cargo, ssh, and any other command that
    has bash completion support.
    
    Note: The '# prototype:' comment must start at the beginning of the line (column 0).
    
    These are registered as "module@prototype" in the completion registry, allowing
    module-specific completion behavior while maintaining fallback to global prototypes.


BASH COMPLETION

  Bar supports bash completion for rulefiles, rules, functions, and their arguments. Completion
  is automatically installed when you run 'bar init_install'.

  The completion script is installed to '~/.bash_completion/bar_complete'. To enable it,
  either source it in your .bashrc:

   source ~/.bash_completion/bar_complete

  Or if you have bash-completion package installed, it should be automatically loaded.

  Completion works for:

  - bar <TAB>       - completes rulefiles, rules, and functions in current directory
  - please <TAB>    - completes rulefiles, rules, and functions
  - ./bar <TAB>     - completes rulefiles, rules, and functions from local bar

  The completion logic will:

  1.  Find any user rule files in current directory (files containing 'function' or 'rule',
      or with shebang pointing to bar/please)
  2.  Complete available bash functions and rules from default rule file and Bar.d/ modules
  3.  Provide context-aware argument completion based on parameter types


LICENSE

    bar -- BAsh Rulez
    Copyright (C) 2025  Christian Thäter <ct.bar@pipapo.org>

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Affero General Public License as
    published by the Free Software Foundation, either version 3 of the
    License, or (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Affero General Public License for more details.

    You should have received a copy of the GNU Affero General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.
